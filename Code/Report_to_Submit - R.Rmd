---
title: "Report_to_submit_R"
author: "Ngoc Uyen PHUNG"
date: "2023-11-27"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Presentation:

Our model aims to assist investors in the decision-making process of making a good investment. In other words, a unicorn business that will reach 5 billion \$ is considered as a wise choice to invest in, from the data available, their valuations are more like to grow over time so provide a huge ROI. Basically, our model predicts whether it is a good investment (Valuation\>5 billion) or not.

## Loading packages

```{r warning=FALSE, include=FALSE}
# Load the package
library(tidyverse)

library(readxl)
library(countrycode)
library(magrittr) # to use the pipe

# Visualization
library(hrbrthemes) # different themes for ggplot2 
library(paletteer)
library(plotly) # to have dynamic plot

# Machine learning 
library(rpart)
library(rpart.plot)
library(gmodels) 
library(e1071)  # For Naive Bayes

```

## Custom made functions

```{r warning=FALSE, include=FALSE}
names_sort_count = function(data, decreasing=TRUE){
  return(
  data %>% 
    table() %>% 
    sort(decreasing = decreasing) %>% 
    names()
  )
}

cors <- function(df, type="pearson") { 
  # turn all three matrices (r, n, and P into a data frame)
  M <- Hmisc::rcorr(as.matrix(df), type = type)
  # return the three data frames in a list return(Mdf)
  Mdf <- map(M, ~data.frame(.x))
  return(Mdf)
}

formatted_cors <- function(df, type="pearson"){
  cors(df, type) %>%
    map(~rownames_to_column(.x, var="measure1")) %>%
    # format each data set (r,P,n) long
    map(~pivot_longer(.x, cols = -measure1, names_to = "measure2")) %>%
    # merge our three list elements by binding the rows
    bind_rows(.id = "id") %>%
    pivot_wider(names_from = id, values_from = value) %>%
    mutate(sig_p = ifelse(P < .05, T, F), p_if_sig = ifelse(P <.05, P, NA), r_if_sig = ifelse(P <.05, r, NA)) 
}

chisq_test_corr = function(data_frame=unicorns_corelation){
  column_names = colnames(data_frame)
  result_list <- lapply(column_names, function(i) {
    value <- chisq.test(data_frame$target_code, 
                        data_frame[[i]])$p.value
    return(data.frame(measure1 = "target_code", measure2 = i, p_value = value))
  })
  
  result_df <- do.call(rbind, result_list)
  result_df = result_df[order(result_df$p_value),]
  return (result_df)
}

fisher_test_corr = function(data_frame=unicorns_corelation){
  column_names = colnames(data_frame)
  result_list <- lapply(column_names, function(i) {
      value <- fisher.test(factor(data_frame$target_code), 
                    factor(data_frame[[i]]), simulate.p.value = TRUE)$p.value
      return(data.frame(measure1 = "target_code", measure2 = i, p_value = value))
  })
  
  result_df <- do.call(rbind, result_list)
  result_df = result_df[order(result_df$p_value),]
  return (result_df)
}
```

# Data Processing

We took the data from: <https://www.cbinsights.com/research-unicorn-companies>. It contains all the unicorn companies as of september 2023.A unicorn company, or unicorn startup, is a private company with a valuation over \$1 billion. Popular former unicorns include Airbnb, Facebook, and Google.

```{r, warning=FALSE}
# Load the data
unicorn_data <- read_excel("../DataSets/CB-Insights_Global-Unicorn-Club_2023.xlsx")
```

## Cleaning the data

Our first step after loading the dataset is to clean it. Thus we took the following cleaning steps :

-   Making sure that there are no duplicated rows in our data set

-   Making sure that all the columns contain coherent information.

    -   For instance in some rows, for the country columns there were names of cities.

-   And we also made sure that the data is loaded in the right format

```{r warning=FALSE, include=FALSE}
# Renaming Columns
colnames(unicorn_data) <- c('company','valuation_billions','date_added','country','city','industry','select_investors')

# Removing the duplicate company 
unicorn_data = unicorn_data %>% 
  distinct( .keep_all = T)

# Removing useless columns
unicorn_data = drop_na(unicorn_data, valuation_billions)


#Sequoia Capital firm name to avoid error when creating new df "unicorn_investors"
unicorn_data$select_investors <- gsub("and Sequoia Capital China", "Sequoia Capital China", unicorn_data$select_investors)


# Dealing with missing value in "country"
unicorn_data[538, "country"] <- "Singapore"

# Remove "City" column
unicorn_data <- subset(unicorn_data, select = -city)

# Removing ",," in "Select investors" correctly count  number of investors
unicorn_data$select_investors <- gsub(",,", ",", unicorn_data$select_investors)

# Replacing City names in the country variable
unicorn_data$country <- gsub("London", "United Kingdom", unicorn_data$country)
unicorn_data$country <- gsub("Munchen", "Germany", unicorn_data$country)

# Year Conversion 
unicorn_data$date_added = ymd(unicorn_data$date_added)


unicorn_data$industry = factor(unicorn_data$industry)
unicorn_data$country = factor(unicorn_data$country)
```

## Enriching the Data

Once the data was cleaned we were faced with the next data preparation step, enriching the data. Thus, we created the following new columns :

-   Number of Investors : the number of investors each company has, they range from 1 to 4 investors

-   Continent : Based on the country the company is coming from we inferred the continent

-   Age : Number of years the company has been an Unicorn

-   Year_added : The year the company became an Unicorn

Initially these were the only new columns we created. However after we did the correlation analysis and the model building we realized that they were not enough to predict if a company is going to reach the 5 billion target or not.\
Thus, we decided that further analysis regarding the investors column needed to be done in order to engineer new variables that will improve our model. The new columns we created are :

-   Associated companies : The companies that are linked to one another through one or more investors.

-   Associated_count : The number of companies associated (leaving out the company we are looking at)

-   Associated_count_over5B : The number of associated companies that have a valuation over 5 billion.

-   Associated_percentage_over5B : The percentage of associated companies that have a valuation over 5 billion, out of the total associated companies

-   Associated_total_value : the total valuation of the unicorn and the the associated companies

-   Associated_mean_value : the mean valuation of the unicorn and the associated companies

-   Investments_avg_pct_over5B : the average percentage of a Unicorn's associate companies with a valuation over 5 Billion per investor

```{r warning=TRUE, include=FALSE}
# Counting the number of Investors
unicorn_data$Number_of_Investors <- sapply(strsplit(as.character(unicorn_data$select_investors), ","), length)


# continent Values
unicorn_data$continent = countrycode(sourcevar = unicorn_data$country,
                            origin = "country.name",
                            destination = "continent")
unicorn_data$continent = factor(unicorn_data$continent)



# Years since Joined
unicorn_data$age = interval(unicorn_data$date_added, now()) %/% years(1)

# Add column year
unicorn_data$year_added <- year(unicorn_data$date_added)

unicorn_data <- unicorn_data %>%
  mutate(target = 
           ifelse(valuation_billions >= 5, "Over 5 $B",  "Under 5 $B") %>% as.factor()) %>% 
  mutate(target_code = ifelse(valuation_billions >= 5, 1, 0)) %>% 
  # Making Sure each company name is well differentiated from one another
  mutate(company = paste(row.names(unicorn_data), unicorn_data$company))

# Creating an intermediary dataframe for later computations
unicorn_transition <- unicorn_data %>%
  mutate(select_investors = strsplit(select_investors, ",")) %>% 
  unnest(select_investors) %>% 
  mutate(select_investors = str_trim(select_investors)) 

# Creating the investors oriented dataframe for further analysis
unicorn_investors = unicorn_transition %>% 
  summarise(.by = select_investors,
            investements_count=n(),
            investements=list(company), 
            investements_sum_valuation=sum(valuation_billions),
            investements_mean_valuation=mean(valuation_billions), 
            investements_over5B_count=sum(target_code), 
            investements_over5B_pct=investements_over5B_count/investements_count
            )

# Investor average percentage over 5B
associate_investor = unicorn_investors %>% 
  unnest(investements) %>% 
  summarise(.by = investements, 
            investements_avg_pct_over5B = mean(investements_over5B_pct)
            )


# Creating the associate companies trasition dataframe for later compuations 
associate_transition = unicorn_transition %>% 
  # Having two columns containing all the companies per Investor in a list
    summarise(.by = select_investors,
            associated_companies=list(company), 
            company=list(company)
            ) %>%
  # For each company we have their investors and a list of the companies 
  # the investors invested in. 
  unnest(company) %>% 
  # Unnesting the second column containg all the companies resulting in the 
  #duplication of all the other columns
  unnest(associated_companies) %>% 
  # Deleting the company name from the associated_company list
  # For each company, we will delete all the duplicates in the association company
  summarise(.by=company,
            associated_companies = unique(associated_companies)
            )

# Creating associate companies data frame with info regarding the associated companies

associate_companies = associate_transition %>% 
  # Deleting the company name from the associated_company list
  subset(company!=associated_companies) %>% 
  left_join(unicorn_data, by=join_by(associated_companies == company)) %>% 
  summarise(.by=company,
            associated_companies = list(unique(associated_companies)), 
            associated_count= n(),
            associated_count_over5Bill=sum(target_code), 
            associated_pct_over5Bill=associated_count_over5Bill/associated_count
            )

# Creating the associate valuation data frame with info regarding associate company valuation 
associate_valuation = associate_transition %>% 
  left_join(unicorn_data, by=join_by(associated_companies == company)) %>% 
  summarise(.by = company,
            association_total_value = sum(valuation_billions), 
            association_mean_value = mean(valuation_billions)
            )

unicorn_data_complete = unicorn_data %>% 
  left_join(associate_companies, by="company") %>% 
  left_join(associate_valuation, by="company") %>% 
  left_join(associate_investor, by=join_by(company == investements))

unicorn_data_complete = unicorn_data_complete %>% 
  mutate(associated_count = ifelse(
    is.na(associated_count), 
    0, 
    associated_count
    )
  ) %>% 
  mutate(associated_count_over5Bill = ifelse(
    is.na(associated_count_over5Bill), 
    0, 
    associated_count_over5Bill
    )
  ) %>% 
    mutate(associated_pct_over5Bill = ifelse(
    is.na(associated_pct_over5Bill), 
    0, 
    associated_pct_over5Bill
    )
  )
```

# Plots to explore the Data

With the four next plots we wanted to explore the distribution of the unicorn companies in order to have a better understanding of the domain.

```{r}
ggplot(unicorn_data, aes(x = industry)) +
  geom_bar(stat = "count", fill = "steelblue") +
  labs(title = "Distribution of the number of companies per industry") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  xlim(names_sort_count(unicorn_data$industry))

```

When we take a look at the industry of origin of the unicors we can see that most of them are tech companies. This is not surprising taking in consideration the enthusiasm around artificial intelligence and big data.

Coming next are finance, consumer & retail and industry, with nearly 200 companies each.

At the other and of the spectrum, with under 100 unicorns we have media&entertainment and insurance.

```{r}
ggplot(unicorn_data, aes(x = country)) +
  geom_bar(stat = "count", fill = "steelblue") +
  xlim(names_sort_count(unicorn_data$country))+
  labs(title = "Distribution of the number of companies per country") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

In regards to the distribution of the companies per country, the US has a clear advance on the other countries with more than 600 unicorns, which is half of the total population of unicorns. On second place we have China with less then 200 companies, folowed by India with less then 100 companies.

```{r}
ggplot(unicorn_data, aes(x = continent)) +
  geom_bar(stat = "count", fill = "steelblue") +
  xlim(names_sort_count(unicorn_data$continent))+
  labs(title = "Distribution of the number of companies per country") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

As expected, the continent with the most unicorns is the Americas, with the huge contribution of the US. On second place we have Asia with the contribution of China and India followed by Europe led by the Uk, Germany and France.

```{r}
ggplot(unicorn_data, aes(x = Number_of_Investors, fill = industry)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "Distribution of the number of investors")+
  scale_fill_paletteer_d("awtools::a_palette")
```

This plot shows that most unicorn companies have 3 investors but very little 4 investors. This is true across all industries. The unicorn with 1 or 2 investors are a minority among the present population.

# Correlation : Pearson and Spearman

```{r include=FALSE}
unicorns_corelation = unicorn_data_complete %>% 
  select(-c(date_added, select_investors, associated_companies, valuation_billions)) %>% 
  mutate(country_code = as.numeric(country)) %>% 
  mutate(industry_code = as.numeric(industry)) %>%
  mutate(continent_code = as.numeric(continent))

unicorns_corelation %>% 
  summary()
  
unicorns_corelation = unicorns_corelation %>% 
  select(-c(company, country, industry, target, continent))
```

## Correlation Matrix

```{r, fig.height=6.4, fig.width=8}
formatted_cors(unicorns_corelation) %>% 
  ggplot(aes(measure1, measure2, fill=r, label=round(r_if_sig,2))) +
  geom_tile() +
  labs(x = NULL, y = NULL, fill = "Pearson's\nCorrelation", title="Correlations in Unicorn_Corelation", subtitle="Only significant Pearson's correlation coefficients shown") +
  scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446", limits=c(-1,1)) +
  geom_text() +
  theme_classic() +
  scale_x_discrete(expand=c(0,0)) +
  scale_y_discrete(expand=c(0,0)) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

```{r, fig.height=6.4, fig.width=8}
formatted_cors(unicorns_corelation, "spearman") %>% 
  ggplot(aes(measure1, measure2, fill=r, label=round(r_if_sig,2))) +
  geom_tile() +
  labs(x = NULL, y = NULL, fill = "Spearman's\nCorrelation", title="Correlations in Unicorn_Corelation", subtitle="Only significant Spearman's correlation coefficients shown") +
  scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446", limits=c(-1,1)) +
  geom_text() +
  theme_classic() +
  scale_x_discrete(expand=c(0,0)) +
  scale_y_discrete(expand=c(0,0)) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

# Corelation : Khi2 and Fisher

```{r}
chisq_test_data = chisq_test_corr() 

chisq_test_data %>% 
  ggplot(aes(x=measure2, y=p_value)) +
  geom_segment( aes(x=measure2, xend=measure2, y=0, yend=p_value), color="skyblue") +
  geom_point( color="blue", size=4, alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  scale_y_continuous(breaks = seq(0, 1, 0.1), labels = scales::percent_format(scale = 1)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red")+
  xlim(chisq_test_data$measure2)+
  labs(x = NULL, title="Khi2 correlations between all varaibles and Target")

```

```{r}
fisher_test_data = fisher_test_corr() 

fisher_test_data %>% 
  ggplot(aes(x=measure2, y=p_value)) +
  geom_segment( aes(x=measure2, xend=measure2, y=0, yend=p_value), color="skyblue") +
  geom_point( color="blue", size=4, alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  scale_y_continuous(breaks = seq(0, 1, 0.1), labels = scales::percent_format(scale = 1)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red")+
  xlim(fisher_test_data$measure2)+
  labs(x = NULL, title="Fisher correlations between all variables and Target")
```

# Model Building

## Dataset splitting

```{r}
# Training set and test set
set.seed(100)
row.number <- sample(1:nrow(unicorn_data_complete), 0.8*nrow(unicorn_data_complete))
train=unicorn_data_complete[row.number,]
test=unicorn_data_complete[-row.number,]

# Splitting efficiency check 

prop.table(table(unicorn_data_complete$target))
prop.table(table(train$target))
prop.table(table(test$target))

```

Overall we have a higher proportion of Under 5 \$B, that evenly distributed in each sets.

# Logistic Regression

## Train

```{r}
logit_model <- glm (target ~year_added + age + Number_of_Investors + 
    associated_count + associated_count_over5Bill + associated_pct_over5Bill + 
    association_total_value + association_mean_value + investements_avg_pct_over5B, 
                      data=train,binomial(link="logit"))
summary(logit_model)
```

## Performance evaluation

```{r}
logit_prediction_test <- predict(logit_model, newdata = test, type = "response")

prop.table(table(logit_prediction_test >0.5,test$target))

CrossTable(logit_prediction_test>0.5,test$target, prop.chisq=FALSE,
chisq=FALSE,prop.t=FALSE,dnn = c("Predicted","Actual"))
```

# Decision Tree

##Train Decision Tree model

```{r}
decision_tree <- rpart(target~year_added + age + Number_of_Investors + associated_count + associated_count_over5Bill + associated_pct_over5Bill + association_total_value + association_mean_value + investements_avg_pct_over5B,
method="class", data=train,
control=rpart.control(minsplit=1),
parms=list(split="information"))
```

## Plot the decision tree

```{r}
rpart.plot(decision_tree, type=2, extra=1)
```

```{r}
tree_prediction_test <- predict(decision_tree,newdata=test,type="class")
```

```{r}
# Confusion matrix for decision tree
confusion_tree <- table(tree_prediction_test, test$target)

# Display confusion matrix
print(confusion_tree)

# Calculate accuracy for decision tree
accuracy_tree <- sum(diag(confusion_tree))/sum(confusion_tree)
print(paste("Accuracy for Decision Tree:", accuracy_tree))

# CrossTable function for a more detailed analysis
CrossTable(tree_prediction_test, test$target, prop.chisq=FALSE,
           chisq=FALSE, prop.t=FALSE, dnn = c("Predicted", "Actual"))
```

# NaÃ¯ve Bayes

## Dataset splitting

```{r}
unicorn_model = unicorn_data_complete %>% 
  select(c(target, age, associated_pct_over5Bill, association_mean_value, association_total_value, investements_avg_pct_over5B, Number_of_Investors, associated_count, associated_count_over5Bill, year_added))

```

```{r}
# Training set and test set
set.seed(100)
row.number <- sample(1:nrow(unicorn_model), 0.8*nrow(unicorn_model))
train=unicorn_model[row.number,]
test=unicorn_model[-row.number,]

# Splitting efficiency check 

prop.table(table(unicorn_model$target))
prop.table(table(train$target))
prop.table(table(test$target))
```

## Train the Naive Bayes model

```{r}
naive_bayes_model <- naiveBayes(target ~ year_added + age + Number_of_Investors + 
                                  associated_count + associated_count_over5Bill + 
                                  associated_pct_over5Bill + association_total_value + 
                                  association_mean_value + investements_avg_pct_over5B, 
                                data = train)

predictions <- predict(naive_bayes_model, newdata = test)

```

```{r}

# Create a confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test$target)
# Convert the confusion matrix to a data frame for plotting
conf_matrix_df <- as.data.frame(confusion_matrix)
# Assign row names as a new column in the data frame
conf_matrix_df$Actual <- rownames(conf_matrix_df)

# Print the confusion matrix
print(confusion_matrix)

# Calculate and print the accuracy of the model
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(paste("Accuracy: ", accuracy))
```

```{r}
# Train a new Naive Bayes model with the different predictors
naive_bayes_model_new <- naiveBayes(target ~ age + Number_of_Investors + 
                                  associated_pct_over5Bill + association_total_value + 
                                  association_mean_value + investements_avg_pct_over5B, 
                                data = train)
predictions <- predict(naive_bayes_model, newdata = test)
# Create a confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test$target)
# Convert the confusion matrix to a data frame for plotting
conf_matrix_df <- as.data.frame(confusion_matrix)
# Assign row names as a new column in the data frame
conf_matrix_df$Actual <- rownames(conf_matrix_df)

# Print the confusion matrix
print(confusion_matrix)

# Calculate and print the accuracy of the model
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(paste("Accuracy: ", accuracy))

```
